install.packages("xgboost")
library(xgboost)
as.matrix(df)
# test model xgboost #
cv <-
xgb.cv(
data = as.matrix(df),
label = df$subs_value,
nrounds = 100,
nfold = 5,
objective = "reg:linear",
eta = 0.3,
max_depth = 6,
early_stopping_rounds = 10,
verbose = 0
)
elog <- cv$evaluation_log
elog %>%
summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min(train_rmse_mean)
ntrees.test  = which.min(test_rmse_mean))
subs_model_xgb <- xgboost(data = as.matrix(df),
label = df$subs_value,
nrounds = 20,
objective = "reg:linear",
eta = 0.3,
depth = 6,
verbose = 0)
subs_model_xgb
subs_model_rf
sqrt(0.004287928)
data("stanford2")
source('P:/1209104-solutions/JDS_ML/github_repo_JDS_ML/JDS_ML/emerging_sub_worldbank_R/scripts/Cleaning_data.R', encoding = 'UTF-8', echo=TRUE)
source('P:/1209104-solutions/JDS_ML/github_repo_JDS_ML/JDS_ML/emerging_sub_worldbank_R/scripts/Cleaning_data.R', encoding = 'UTF-8', echo=TRUE)
summarry(data$emission_air)
summary(data$emission_air)
summary(data$emission_ww)
summary(data$emission_water)
summary(data$emission_soil)
# Loading libraries -------------------------------------------------------
library(tidyverse)
library(FactorMiner)
install.packages("FactorMiner")
library(FactoMiner)
install.packages("FactoMiner")
library(FactoMineR)
df <- read_rds("data/modified/compact_data.rds")
names(data)
names(df)
# PCA #
pca_df <-
select(
df,
-SUBID,
-CountryCorrFinal,
-country_nr,
-LAKEREGION,
-POURX,
-POURY,
-TARGETX,
-TARGETY,
-CENTERX,
-CENTERY,
-LATITUDE,
-LONGITUDE,
-station_co,
-Substance,
-CAS_No,
-CAS_No,
-H_Unit,
-Concentration,
-valid_measurement
)
pca_all <- pca_output_all <- PCA(pca_df, quanti.sup = 1:8, quali.sup = 20:21, graph = )
names(pca_df)
pca_all <- pca_output_all <- PCA(pca_df, quanti.sup = 99:101, quali.sup = 102, graph = TRUE)
dimdesc(pca_all, axes = 1:2)
pca_all$var
pca_all$eig[,2][1:5]
pca_all$eig[,3][1:5]
pca_all$eig[,2][1:20]
pca_all$eig[,3][1:20]
summary(pca_all, nbelements = 100)
summary(pca_all)
?PCA
library(factoextra)
fviz_cos2(pca_all, choice = "var", axes = 1, top = 10)
fviz_cos2(pca_all, choice = "var", axes = 2, top = 10)
fviz_contrib(pca_all, choice = "var", axes = 1, top = 10)
fviz_contrib(pca_all, choice = "var", axes = 2, top = 10)
# Loading libraries -------------------------------------------------------
library(FactoMineR)
library(factoextra)
library(paran)
library(psych)
library(tidyverse)
# Loading data ------------------------------------------------------------
df <- read_rds("data/modified/compact_data.rds")
# PCA #
pca_df <-
select(
df,
-SUBID,
-CountryCorrFinal,
-country_nr,
-LAKEREGION,
-POURX,
-POURY,
-TARGETX,
-TARGETY,
-CENTERX,
-CENTERY,
-LATITUDE,
-LONGITUDE,
-station_co,
-Substance,
-CAS_No,
-CAS_No,
-H_Unit,
-Concentration,
-valid_measurement
)
pca_all <- PCA(pca_df, quanti.sup = 99:101, quali.sup = 102, graph = TRUE)
summary(pca_all)
dimdesc(pca_all, axes = 1:2)
pca_all$eig[,2][1:30]
pca_all$eig[,3][1:30]
pca_all$var
fviz_cos2(pca_all, choice = "var", axes = 1, top = 10)
fviz_cos2(pca_all, choice = "var", axes = 2, top = 10)
fviz_contrib(pca_all, choice = "var", axes = 1, top = 10)
fviz_contrib(pca_all, choice = "var", axes = 2, top = 10)
fviz_screeplot(pca_all, ncp = 20)
get_eigenvalue(pca_all)
select(pca_df, ks)
pca_df
select(pca_df, subs_value, AREA:pest_bin)
sum(is.na(select(pca_df, subs_value, AREA:pest_bin)))
# parallel analysis #
paran_df <- select(pca_df, subs_value, AREA:pest_bin)
paran(paran_df, graph = TRUE)
fa.parallel(paran_df)
sum(is.na(paran_df))
sum(is.infinite(paran_df))
paran_df %>% mutate_if(is.numeric, list(~na_if(., Inf)))
sum(is.na(paran_df))
paran_df %>% mutate_if(is.numeric, list(~na_if(., Inf))) %>% is.na()
paran_df %>% mutate_if(is.numeric, list(~na_if(., Inf))) %>% is.na() %>% sum()
paran_df %>% mutate_if(is.numeric, list(~na_if(., -Inf))) %>% is.na() %>% sum()
# parallel analysis #
paran_df <- select(pca_df, subs_value, AREA:pest_bin)
any(is.na(paran_df))
any(is.infinite(paran_df))
apply(paran_df, 2, function(x) any(is.infinite(x)))
# parallel analysis #
paran_df <- select(pca_df, subs_value, AREA:CumAreakkm2)
paran(paran_df, graph = TRUE)
# parallel analysis #
paran_df <- complete.cases(select(pca_df, subs_value, AREA:CumAreakkm2))
paran(paran_df, graph = TRUE)
fa.parallel(paran_df)
# parallel analysis #
paran_df <- select(pca_df, subs_value, AREA:CumAreakkm2)
paran_df
# parallel analysis #
paran_df <- as.dataframe(select(pca_df, subs_value, AREA:CumAreakkm2))
# parallel analysis #
paran_df <- as.data.frame(select(pca_df, subs_value, AREA:CumAreakkm2))
paran(paran_df, graph = TRUE)
# parallel analysis #
paran_df <- as.data.frame(complete.cases(select(pca_df, subs_value, AREA:CumAreakkm2)))
paran(paran_df, graph = TRUE)
fa.parallel(paran_df)
?paran
paran(pca_all, graph = TRUE)
fa.parallel(pca_all)
pca_all
as.matrix(paran_df)
paran(as.matrix(paran_df), graph = TRUE)
apply(paran_df, 2, function(x) any(is.infinite(x)))
paran_df
# parallel analysis #
paran_df <- as.data.frame(complete.cases(select(pca_df, subs_value, AREA:CumAreakkm2)))
paran_df
select(pca_df, subs_value, AREA:CumAreakkm2)
complete.cases(select(pca_df, subs_value, AREA:CumAreakkm2))
# parallel analysis #
paran_df <- select(pca_df, subs_value, AREA:CumAreakkm2)
paran_df
paran_df <- as.data.frame(paran_df[complete.cases(paran_df), ])
paran_df
paran(paran_df, graph = TRUE)
paran(as.matrix(paran_df), graph = TRUE)
paran(paran_df, graph = TRUE)
get_eigenvalue(pca_all)
pca_all$eig[,2][1:30]
summary(pca_all)
pca_all
str(pca_all)
install.packages("Rtsne")
library(Rtsne)
# t-SNE
tsne_df <- select(pca_df, AREA:CumAreakkm2)
tsne_output <- Rtsne(tsne_df, perplexity = 50, max_iter = 1200, dims = 3)
# Loading libraries -------------------------------------------------------
library(FactoMineR)
library(factoextra)
library(paran)
library(psych)
library(tidyverse)
library(Rtsne)
# Loading data ------------------------------------------------------------
df <- read_rds("data/modified/compact_data.rds")
# PCA #
pca_df <-
select(
df,
-SUBID,
-CountryCorrFinal,
-country_nr,
-LAKEREGION,
-POURX,
-POURY,
-TARGETX,
-TARGETY,
-CENTERX,
-CENTERY,
-LATITUDE,
-LONGITUDE,
-station_co,
-Substance,
-CAS_No,
-CAS_No,
-H_Unit,
-Concentration,
-valid_measurement
)
pca_all <- PCA(pca_df, quanti.sup = 99:101, quali.sup = 102, graph = TRUE)
summary(pca_all)
dimdesc(pca_all, axes = 1:2)
pca_all$eig[,2][1:30]
pca_all$eig[,3][1:30]
pca_all$var
fviz_cos2(pca_all, choice = "var", axes = 1, top = 10)
fviz_cos2(pca_all, choice = "var", axes = 2, top = 10)
fviz_contrib(pca_all, choice = "var", axes = 1, top = 10)
fviz_contrib(pca_all, choice = "var", axes = 2, top = 10)
fviz_screeplot(pca_all, ncp = 20)
get_eigenvalue(pca_all)
# parallel analysis #
paran_df <- select(pca_df, subs_value, AREA:CumAreakkm2)
paran_df <- as.data.frame(paran_df[complete.cases(paran_df), ])
paran(paran_df, graph = TRUE)
fa.parallel(paran_df)
# t-SNE #
tsne_df <- select(pca_df, AREA:CumAreakkm2)
tsne_output <- Rtsne(tsne_df, perplexity = 50, max_iter = 1200, dims = 3)
tsne_df[duplicated(tsne_df), ]
tsne_df[unique(tsne_df), ]
tsne_df[duplicated(tsne_df), ]
tsne_df[duplicated(paran_df), ]
tsne_output <- Rtsne(paran_df, perplexity = 50, max_iter = 1200, dims = 3)
tsne_output <- Rtsne(paran_df, perplexity = 50, max_iter = 1200, dims = 3, check_duplicates = FALSE)
tsne_output
# Loading libraries -------------------------------------------------------
library(tidyverse)
library(naniar)
library(ranger)
library(xgboost)
# Loading data ------------------------------------------------------------
df <- read_rds("data/modified/compact_data.rds")
# Cleaning data for feature table #
df <- select(df, -SUBID:-Concentration, -valid_measurement, -sub_groups)
# first test removing al missing values (for now) #
df <- na.omit(df)
library(vtreat)
install.packages("vtreat")
library(vtreat)
N <- nrow(df)
target <- round(0.75 * N)
gp <- runif(N)
# splitting the data
df_train <- df[gp < 0.75, ]
df_test <- df[gp >= 0.75, ]
fmla <- as.formula("subs_value ~ .")
# Cross validation Plan #
nRows <- nrow(df)
splitPlan <- kWayCrossValidation(nRows, 3, NULL, NULL)
str(splitPlan)
fmla <- as.formula("subs_value ~ .")
# test model  random forrest #
(subs_model_rf <-
ranger(
formula = fmla,
data = df_train,
num.trees = 500,
respect.unordered.factors = "order"
))
df_train$rf_pred <- predict(subs_model_rf, df_test)$predictions
df_test$rf_pred <- predict(subs_model_rf, df_test)$predictions
mutate(df_test , residual = subs_value - rf_pred) %>%
summarise(rmse = sqrt(mean(residual ^2)))
ggplot(df_test, aes(x = rf_pred, y = subs_value)) +
geom_point() +
geom_abline()
df_train[, -"subs_value"]
df_train[, -df_train$subs_value]
select(df_train, -subs_value)
select(df_test, -subs_value)
df_train_xg <- select(df_train, -subs_value)
df_test_xg <- select(df_test, -subs_value)
# test model xgboost #
df_train_xg <- select(df_train, -subs_value)
df_test_xg <- select(df_test, -subs_value)
cv <-
xgb.cv(
data = as.matrix(df_train_xg),
label = df_train$subs_value,
nrounds = 100,
nfold = 5,
objective = "reg:linear",
eta = 0.3,
max_depth = 6,
early_stopping_rounds = 10,
verbose = 0
)
elog <- cv$evaluation_log
elog %>%
summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min (train_rmse_mean)
ntrees.test  = which.min(test_rmse_mean))
elog <- cv$evaluation_log
elog %>%
summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min (train_rmse_mean)
ntrees.test  = which.min(test_rmse_mean))
subs_model_xgb <- xgboost(data = as.matrix(df_train_xg),
label = df_train$subs_value,
nrounds = 15,
objective = "reg:linear",
eta = 0.3,
depth = 6,
verbose = 0)
df_test$xgb_pred <- predict(subs_model_xgb, df_test)$predictions
df_test$xgb_pred <- predict(subs_model_xgb, as.matrix(df_test_xg))$predictions
df_test$xgb_pred <- predict(subs_model_xgb, as.matrix(df_test_xg))
df_test
rf_pred <- df_test$rf_pred
df_test <- select(df_test, -rf_pred)
df_test <- select(df_test, -rf_pred)
rf_pred <- df_test$rf_pred
df_test <- select(df_test, -rf_pred)
rf_pred
# plot model performance
ggplot(df_test, aes(x = rf_pred, y = subs_value)) +
geom_point() +
geom_abline()
# Loading libraries -------------------------------------------------------
library(tidyverse)
library(naniar)
library(vtreat)
library(ranger)
library(xgboost)
# Loading data ------------------------------------------------------------
df <- read_rds("data/modified/compact_data.rds")
# Cleaning data for feature table #
df <- select(df, -SUBID:-Concentration, -valid_measurement, -sub_groups)
# first test removing al missing values (for now) #
df <- na.omit(df)
# create test and training data #
N <- nrow(df)
target <- round(0.75 * N)
gp <- runif(N)
# splitting the data
df_train <- df[gp < 0.75, ]
df_test <- df[gp >= 0.75, ]
# Cross validation Plan #
nRows <- nrow(df)
splitPlan <- kWayCrossValidation(nRows, 3, NULL, NULL)
fmla <- as.formula("subs_value ~ .")
# test model  random forrest #
(subs_model_rf <-
ranger(
formula = fmla,
data = df_train,
num.trees = 500,
respect.unordered.factors = "order"
))
df_test$rf_pred <- predict(subs_model_rf, df_test)$predictions
# RMSE on test data
mutate(df_test, residual = subs_value - rf_pred) %>%
summarise(rmse = sqrt(mean(residual ^2)))
# plot model performance
ggplot(df_test, aes(x = rf_pred, y = subs_value)) +
geom_point() +
geom_abline()
rf_pred <- df_test$rf_pred
rf_pred
df_test <- select(df_test, -rf_pred)
# test model xgboost #
df_train_xg <- select(df_train, -subs_value)
df_test_xg <- select(df_test, -subs_value)
cv <-
xgb.cv(
data = as.matrix(df_train_xg),
label = df_train$subs_value,
nrounds = 100,
nfold = 5,
objective = "reg:linear",
eta = 0.3,
max_depth = 6,
early_stopping_rounds = 10,
verbose = 0
)
elog <- cv$evaluation_log
elog %>%
summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min (train_rmse_mean)
ntrees.test  = which.min(test_rmse_mean))
subs_model_xgb <- xgboost(data = as.matrix(df_train_xg),
label = df_train$subs_value,
nrounds = 15,
objective = "reg:linear",
eta = 0.3,
depth = 6,
verbose = 0)
df_test$xgb_pred <- predict(subs_model_xgb, as.matrix(df_test_xg))
mutate(df_test, residual = subs_value - xgb_pred) %>%
summarise(rmse = sqrt(mean(residual ^2)))
ggplot(df_test, aes(x = xgb_pred, y = subs_value)) +
geom_point() +
geom_abline()
df_test$rf_pred <- rf_pred
mutate(df_test, residual_rf = subs_value - rf_pred,
residual_xgb = subs_value - xgb_pred) %>%
summarise(rmse_rf = sqrt(mean(residual_rf ^2)),
rmse_xgb = sqrt(mean(residual_xgb ^2)))
# Loading libraries -------------------------------------------------------
library(tidyverse)
library(naniar)
library(vtreat)
library(ranger)
library(xgboost)
# Loading data ------------------------------------------------------------
df <- read_rds("data/modified/compact_data.rds")
# Cleaning data for feature table #
df <- select(df, -SUBID:-Concentration, -valid_measurement, -sub_groups)
# first test removing al missing values (for now) #
df <- na.omit(df)
# create test and training data #
N <- nrow(df)
target <- round(0.75 * N)
gp <- runif(N)
# splitting the data
df_train <- df[gp < 0.75, ]
df_test <- df[gp >= 0.75, ]
# Cross validation Plan #
nRows <- nrow(df)
splitPlan <- kWayCrossValidation(nRows, 3, NULL, NULL)
fmla <- as.formula("subs_value ~ .")
# test model  random forrest #
(subs_model_rf <-
ranger(
formula = fmla,
data = df_train,
num.trees = 500,
respect.unordered.factors = "order"
))
df_test$rf_pred <- predict(subs_model_rf, df_test)$predictions
# RMSE on test data
mutate(df_test, residual = subs_value - rf_pred) %>%
summarise(rmse = sqrt(mean(residual ^2)))
# plot model performance
ggplot(df_test, aes(x = rf_pred, y = subs_value)) +
geom_point() +
geom_abline()
rf_pred <- df_test$rf_pred
df_test <- select(df_test, -rf_pred)
# test model xgboost #
df_train_xg <- select(df_train, -subs_value)
df_test_xg <- select(df_test, -subs_value)
cv <-
xgb.cv(
data = as.matrix(df_train_xg),
label = df_train$subs_value,
nrounds = 100,
nfold = 5,
objective = "reg:linear",
eta = 0.3,
max_depth = 6,
early_stopping_rounds = 10,
verbose = 0
)
elog <- cv$evaluation_log
elog %>%
summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min (train_rmse_mean)
ntrees.test  = which.min(test_rmse_mean))
subs_model_xgb <- xgboost(data = as.matrix(df_train_xg),
label = df_train$subs_value,
nrounds = 15,
objective = "reg:linear",
eta = 0.3,
depth = 6,
verbose = 0)
df_test$xgb_pred <- predict(subs_model_xgb, as.matrix(df_test_xg))
mutate(df_test, residual = subs_value - xgb_pred) %>%
summarise(rmse = sqrt(mean(residual ^2)))
ggplot(df_test, aes(x = xgb_pred, y = subs_value)) +
geom_point() +
geom_abline()
df_test$rf_pred <- rf_pred
# check both models #
mutate(df_test, residual_rf = subs_value - rf_pred,
residual_xgb = subs_value - xgb_pred) %>%
summarise(rmse_rf = sqrt(mean(residual_rf ^2)),
rmse_xgb = sqrt(mean(residual_xgb ^2)))
