-log_kow,
-molar_mass,
-ks,
-sub_groups
)
?PCA
dbDisconnect()
pca_all <- PCA(pca_df, quanti.sup = 99:101, quali.sup = 102, graph = TRUE)
pca_base <- prcomp(pca_base_df, scale. = TRUE)
summary(pca_base_df)
source('P:/1209104-solutions/JDS_ML/github_repo_JDS_ML/JDS_ML/emerging_sub_worldbank_R/scripts/Cleaning_data.R', encoding = 'UTF-8', echo=TRUE)
source('P:/1209104-solutions/JDS_ML/github_repo_JDS_ML/JDS_ML/emerging_sub_worldbank_R/scripts/Cleaning_data.R', encoding = 'UTF-8', echo=TRUE)
# Loading libraries -------------------------------------------------------
library(FactoMineR)
library(factoextra)
library(paran)
library(psych)
library(Rtsne)
library(tidyverse)
# Loading data ------------------------------------------------------------
df <- read_rds("data/modified/compact_data.rds")
# PCA #
pca_df <-
select(
df,
-SUBID,
-CountryCorrFinal,
-country_nr,
-LAKEREGION,
-POURX,
-POURY,
-TARGETX,
-TARGETY,
-CENTERX,
-CENTERY,
-LATITUDE,
-LONGITUDE,
-station_co,
-Substance,
-CAS_No,
-CAS_No,
-H_Unit,
-Concentration,
-valid_measurement
)
pca_base_df <-
select(
pca_df,
-emission_air,
-emission_water,
-emission_ww,
-emission_soil,
-kbiodeg,
-log_kow,
-molar_mass,
-ks,
-sub_groups
)
pca_base <- prcomp(pca_base_df, scale. = TRUE)
names(pca_df)
pca_all <- PCA(pca_df, quanti.sup = 78:80, quali.sup = 81, graph = TRUE)
pca_base$sdev
pca_base$sdev ^2
pca_base$sdev ^2 %>% round(2)
pca_base$rotation
fviz_screeplot(pca_all, ncp = 20)
fviz_screeplot(pca_base, ncp = 20)
paran(pca_base, graph = TRUE)
paran(pca_base$x, graph = TRUE)
summary(pca_base)
pca_base$sdev 62
pca_base$sdev ^2
# Loading libraries -------------------------------------------------------
library(FactoMineR)
library(factoextra)
library(paran)
library(psych)
library(Rtsne)
library(tidyverse)
# Loading data ------------------------------------------------------------
df <- read_rds("data/modified/compact_data.rds")
# PCA #
pca_df <-
select(
df,
-SUBID,
-CountryCorrFinal,
-country_nr,
-LAKEREGION,
-POURX,
-POURY,
-TARGETX,
-TARGETY,
-CENTERX,
-CENTERY,
-LATITUDE,
-LONGITUDE,
-station_co,
-Substance,
-CAS_No,
-CAS_No,
-H_Unit,
-Concentration,
-valid_measurement
)
pca_base_df <-
select(
pca_df,
-emission_air,
-emission_water,
-emission_ww,
-emission_soil,
-kbiodeg,
-log_kow,
-molar_mass,
-ks,
-sub_groups
)
# Loading libraries -------------------------------------------------------
library(FactoMineR)
library(factoextra)
library(paran)
library(psych)
library(Rtsne)
library(tidyverse)
# Loading data ------------------------------------------------------------
df <- read_rds("data/modified/compact_data.rds")
df
glimpse(df)
source('P:/1209104-solutions/JDS_ML/github_repo_JDS_ML/JDS_ML/emerging_sub_worldbank_R/scripts/Cleaning_data.R', encoding = 'UTF-8', echo=TRUE)
# Loading libraries -------------------------------------------------------
library(tidyverse)
library(naniar)
library(vtreat)
library(ranger)
library(xgboost)
# Loading data ------------------------------------------------------------
df <- read_rds("data/modified/compact_data.rds")
# Cleaning data for feature table #
df <- select(df, -SUBID:-Concentration, -valid_measurement, -sub_groups)
# first test removing al missing values (for now) #
df <- na.omit(df)
# create test and training data #
N <- nrow(df)
target <- round(0.75 * N)
gp <- runif(N)
# splitting the data
df_train <- df[gp < 0.75, ]
df_test <- df[gp >= 0.75, ]
# Cross validation Plan #
nRows <- nrow(df)
splitPlan <- kWayCrossValidation(nRows, 3, NULL, NULL)
fmla <- as.formula("subs_value ~ .")
# test model  random forrest #
(subs_model_rf <-
ranger(
formula = fmla,
data = df_train,
num.trees = 500,
respect.unordered.factors = "order"
))
df_test$rf_pred <- predict(subs_model_rf, df_test)$predictions
# RMSE on test data
mutate(df_test, residual = subs_value - rf_pred) %>%
summarise(rmse = sqrt(mean(residual ^2)))
# plot model performance
ggplot(df_test, aes(x = rf_pred, y = subs_value)) +
geom_point() +
geom_abline()
rf_pred <- df_test$rf_pred
df_test <- select(df_test, -rf_pred)
# test model xgboost #
df_train_xg <- select(df_train, -subs_value)
df_test_xg <- select(df_test, -subs_value)
cv <-
xgb.cv(
data = as.matrix(df_train_xg),
label = df_train$subs_value,
nrounds = 100,
nfold = 5,
objective = "reg:linear",
eta = 0.3,
max_depth = 6,
early_stopping_rounds = 10,
verbose = 0
)
elog <- cv$evaluation_log
elog %>%
summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min (train_rmse_mean)
ntrees.test  = which.min(test_rmse_mean))
subs_model_xgb <- xgboost(data = as.matrix(df_train_xg),
label = df_train$subs_value,
nrounds = 15,
objective = "reg:linear",
eta = 0.3,
depth = 6,
verbose = 0)
df_test$xgb_pred <- predict(subs_model_xgb, as.matrix(df_test_xg))
mutate(df_test, residual = subs_value - xgb_pred) %>%
summarise(rmse = sqrt(mean(residual ^2)))
ggplot(df_test, aes(x = xgb_pred, y = subs_value)) +
geom_point() +
geom_abline()
df_test$rf_pred <- rf_pred
# check both models #
mutate(df_test, residual_rf = subs_value - rf_pred,
residual_xgb = subs_value - xgb_pred) %>%
summarise(rmse_rf = sqrt(mean(residual_rf ^2)),
rmse_xgb = sqrt(mean(residual_xgb ^2)))
library(caret)
install.packages("caret")
library(caret)
?trainControl
glmnet_control <- trainControl(
method = "cv",
number = 10,
)
glmnet_model <- train(
subs_value ~ .,
data = df_train,
method = "glmnet",
trControl = glmnet_control
)
glmnet_model <- train(
subs_value ~ .,
data = df_train,
method = "glmnet",
trControl = glmnet_control
)
glmnet_model
plot(glmnet_model)
glmnet_control <- trainControl(
method = "cv",
number = 10,
)
glmnet_grid <- expand.grid(
alpha = 0:1,
lambda = seq(0.0001, 0.1, length = 10)
)
# glmnet #
glmnet_control <- trainControl(
method = "cv",
number = 10,
)
glmnet_grid <- expand.grid(
alpha = 0:1,
lambda = seq(0.0001, 0.1, length = 10)
)
glmnet_model <- train(
subs_value ~ .,
data = df_train,
method = "glmnet",
tuneGrid = glmnet_grid,
trControl = glmnet_control
)
glmnet_model
glmnet_model %>% plot()
install.packages(c("arrow", "naniar", "RSQLite"))
install.packages(c("factoextra", "FactoMineR", "paran", "psych", "Rtsne"))
install.packages(c("caret", "h2o", "naniar", "ranger", "tictoc", "vtreat", "xgboost"))
# Loading libraries -------------------------------------------------------
library(tidyverse)
library(naniar)
library(vtreat)
library(ranger)
library(xgboost)
library(caret)
library(tictoc)
library(h2o)
# Loading data ------------------------------------------------------------
df <- read_rds("data/modified/compact_data.rds")
# Cleaning data for feature table #
df <- select(df, -SUBID:-Concentration, -valid_measurement, -sub_groups)
# first test removing al missing values (for now) #
df <- na.omit(df)
# create test and training data #
set.seed(1234)
N <- nrow(df)
target <- round(0.75 * N)
gp <- runif(N)
# splitting the data
df_train <- df[gp < 0.75, ]
df_test <- df[gp >= 0.75, ]
# Cross validation Plan #
nRows <- nrow(df)
splitPlan <- kWayCrossValidation(nRows, 3, NULL, NULL)
fmla <- as.formula("subs_value ~ .")
# test model  random forrest #
(subs_model_rf <-
ranger(
formula = fmla,
data = df_train,
num.trees = 500,
respect.unordered.factors = "order"
))
df_test$rf_pred <- predict(subs_model_rf, df_test)$predictions
# RMSE on test data
mutate(df_test, residual = subs_value - rf_pred) %>%
summarise(rmse = sqrt(mean(residual ^2)))
# plot model performance
ggplot(df_test, aes(x = rf_pred, y = subs_value)) +
geom_point() +
geom_abline()
rf_pred <- df_test$rf_pred
df_test <- select(df_test, -rf_pred)
# test model xgboost #
df_train_xg <- select(df_train, -subs_value)
df_test_xg <- select(df_test, -subs_value)
cv <-
xgb.cv(
data = as.matrix(df_train_xg),
label = df_train$subs_value,
nrounds = 100,
nfold = 5,
objective = "reg:linear",
eta = 0.3,
max_depth = 6,
early_stopping_rounds = 10,
verbose = 0
)
elog <- cv$evaluation_log
elog %>%
summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min (train_rmse_mean)
ntrees.test  = which.min(test_rmse_mean))
subs_model_xgb <- xgboost(data = as.matrix(df_train_xg),
label = df_train$subs_value,
nrounds = 15,
objective = "reg:linear",
eta = 0.3,
depth = 6,
verbose = 0)
df_test$xgb_pred <- predict(subs_model_xgb, as.matrix(df_test_xg))
mutate(df_test, residual = subs_value - xgb_pred) %>%
summarise(rmse = sqrt(mean(residual ^2)))
ggplot(df_test, aes(x = xgb_pred, y = subs_value)) +
geom_point() +
geom_abline()
df_test$rf_pred <- rf_pred
# check both models #
mutate(df_test, residual_rf = subs_value - rf_pred,
residual_xgb = subs_value - xgb_pred) %>%
summarise(rmse_rf = sqrt(mean(residual_rf ^2)),
rmse_xgb = sqrt(mean(residual_xgb ^2)))
# Models with caret -------------------------------------------------------
# glmnet #
fit_control <- trainControl(
method = "adaptive_cv",
adaptive = list(
min = 5,
alpha = 0.05,
method = "BT",
complete = TRUE
),
number = 5,
repeats = 3,
search = "random",
verboseIter = TRUE,
allowParallel = TRUE
)
glmnet_grid <- expand.grid(
alpha = seq(0, 1, 0.1),
lambda = seq(0.0001, 0.1, length = 10)
)
tic()
glmnet_model <- train(
subs_value ~ .,
data = df_train,
method = "glmnet",
tuneLength = 100,
trControl = fit_control
)
install.packages("BradleyTerry2")
# glmnet #
fit_control <- trainControl(
method = "adaptive_cv",
adaptive = list(
min = 5,
alpha = 0.05,
method = "BT",
complete = TRUE
),
number = 5,
repeats = 3,
search = "random",
verboseIter = TRUE,
allowParallel = TRUE
)
glmnet_grid <- expand.grid(
alpha = seq(0, 1, 0.1),
lambda = seq(0.0001, 0.1, length = 10)
)
tic()
glmnet_model <- train(
subs_value ~ .,
data = df_train,
method = "glmnet",
tuneLength = 100,
trControl = fit_control
)
toc()
glmnet_model
glmnet_model$bestTune
glmnet_model$results
glmnet_model$modelInfo
glmnet_model$modelType
glmnet_model$dots
h2o.init()
library(h2o)
h2o.init()
library(h2o)
h2o.init()
df_h2o <- as.h2o(df)
y <- "subs_value"
x <- setdiff(colnames(df_h2o), y)
x
sframe <- h2o.splitFrame(data = df_h2o, ratios = c(0.7, 0.15), seed = 1234)
train <- sframe[[1]]
valid <- sframe[[2]]
test <- sframe[[3]]
h2o.xgboost.available()
automl_model <- h2o.automl(x = x,
y = y,
training_frame = train,
validation_frame = valid,
max_models = 50,
sort_metric = "RMSE",
nfolds = 5,
seed = 1234)
automl_model @leaderboard
# Loading libraries -------------------------------------------------------
library(tidyverse)
library(naniar)
library(vtreat)
library(ranger)
library(xgboost)
library(caret)
library(tictoc)
library(h2o)
# Loading data ------------------------------------------------------------
df <- read_rds("data/modified/compact_data.rds")
# Cleaning data for feature table #
df <- select(df, -SUBID:-Concentration, -valid_measurement, -sub_groups)
# first test removing al missing values (for now) #
df <- na.omit(df)
# create test and training data #
set.seed(1234)
N <- nrow(df)
target <- round(0.75 * N)
gp <- runif(N)
# splitting the data
df_train <- df[gp < 0.75, ]
df_test <- df[gp >= 0.75, ]
# Cross validation Plan #
nRows <- nrow(df)
splitPlan <- kWayCrossValidation(nRows, 3, NULL, NULL)
fmla <- as.formula("subs_value ~ .")
# test model  random forrest #
(subs_model_rf <-
ranger(
formula = fmla,
data = df_train,
num.trees = 500,
respect.unordered.factors = "order"
))
df_test$rf_pred <- predict(subs_model_rf, df_test)$predictions
# RMSE on test data
mutate(df_test, residual = subs_value - rf_pred) %>%
summarise(rmse = sqrt(mean(residual ^2)))
# plot model performance
ggplot(df_test, aes(x = rf_pred, y = subs_value)) +
geom_point() +
geom_abline()
rf_pred <- df_test$rf_pred
df_test <- select(df_test, -rf_pred)
# test model xgboost #
df_train_xg <- select(df_train, -subs_value)
df_test_xg <- select(df_test, -subs_value)
cv <-
xgb.cv(
data = as.matrix(df_train_xg),
label = df_train$subs_value,
nrounds = 100,
nfold = 5,
objective = "reg:linear",
eta = 0.3,
max_depth = 6,
early_stopping_rounds = 10,
verbose = 0
)
elog <- cv$evaluation_log
elog %>%
summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min (train_rmse_mean)
ntrees.test  = which.min(test_rmse_mean))
subs_model_xgb <- xgboost(data = as.matrix(df_train_xg),
label = df_train$subs_value,
nrounds = 15,
objective = "reg:linear",
eta = 0.3,
depth = 6,
verbose = 0)
df_test$xgb_pred <- predict(subs_model_xgb, as.matrix(df_test_xg))
mutate(df_test, residual = subs_value - xgb_pred) %>%
summarise(rmse = sqrt(mean(residual ^2)))
ggplot(df_test, aes(x = xgb_pred, y = subs_value)) +
geom_point() +
geom_abline()
df_test$rf_pred <- rf_pred
# check both models #
mutate(df_test, residual_rf = subs_value - rf_pred,
residual_xgb = subs_value - xgb_pred) %>%
summarise(rmse_rf = sqrt(mean(residual_rf ^2)),
rmse_xgb = sqrt(mean(residual_xgb ^2)))
